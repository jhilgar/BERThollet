token_model: bpe
vocab_size: 512 # including the 4 special tokens

data_directory: ../data/
dataset_directory: ../data/sequences
training_directory: ../data/training

trainingarguments:
  num_train_epochs: 1
  save_steps: 500
  save_total_limit: 2
  prediction_loss_only: True
  remove_unused_columns: True
  logging_steps: 50
  max_steps: 25000000
  per_device_train_batch_size: 3
  optim: adamw_torch
  learning_rate: 0.00001
  adam_epsilon: 0.000001
  warmup_steps: 20000
  fp16: True